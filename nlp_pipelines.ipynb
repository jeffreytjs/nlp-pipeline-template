{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build text classificaton model using a dataset which contains what corporations actually talk about on social media. The statements were labelled as into following categories - `information` (objective statements about the company or it's activities), `dialog` (replies to users, etc.), or `action` (messages that ask for votes or ask users to click on links, etc.). Our aim is to build a model to automatically categorize the text into their respective categories. You can download the dataset from [here](https://data.world/crowdflower/corporate-messaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Understanding and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv('corporate_messaging_dfe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     unit_id  golden unit_state  trusted_judgments     last_judgment_at  \\\n",
       "0  662822308   False  finalized                  3  2015-02-18T04:31:00   \n",
       "1  662822309   False  finalized                  3  2015-02-18T13:55:00   \n",
       "2  662822310   False  finalized                  3  2015-02-18T08:43:00   \n",
       "3  662822311   False  finalized                  3  2015-02-18T09:13:00   \n",
       "4  662822312   False  finalized                  3  2015-02-18T06:48:00   \n",
       "\n",
       "      category  category_confidence category_gold                  id  \\\n",
       "0  Information                  1.0           NaN  436528000000000000   \n",
       "1  Information                  1.0           NaN  386013000000000000   \n",
       "2  Information                  1.0           NaN  379580000000000000   \n",
       "3  Information                  1.0           NaN  367530000000000000   \n",
       "4  Information                  1.0           NaN  360385000000000000   \n",
       "\n",
       "  screenname                                               text  \n",
       "0   Barclays  Barclays CEO stresses the importance of regula...  \n",
       "1   Barclays  Barclays announces result of Rights Issue http...  \n",
       "2   Barclays  Barclays publishes its prospectus for its �5.8...  \n",
       "3   Barclays  Barclays Group Finance Director Chris Lucas is...  \n",
       "4   Barclays  Barclays announces that Irene McDermott Brown ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unit_id</th>\n      <th>golden</th>\n      <th>unit_state</th>\n      <th>trusted_judgments</th>\n      <th>last_judgment_at</th>\n      <th>category</th>\n      <th>category_confidence</th>\n      <th>category_gold</th>\n      <th>id</th>\n      <th>screenname</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>662822308</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>3</td>\n      <td>2015-02-18T04:31:00</td>\n      <td>Information</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>436528000000000000</td>\n      <td>Barclays</td>\n      <td>Barclays CEO stresses the importance of regula...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>662822309</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>3</td>\n      <td>2015-02-18T13:55:00</td>\n      <td>Information</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>386013000000000000</td>\n      <td>Barclays</td>\n      <td>Barclays announces result of Rights Issue http...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>662822310</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>3</td>\n      <td>2015-02-18T08:43:00</td>\n      <td>Information</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>379580000000000000</td>\n      <td>Barclays</td>\n      <td>Barclays publishes its prospectus for its �5.8...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>662822311</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>3</td>\n      <td>2015-02-18T09:13:00</td>\n      <td>Information</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>367530000000000000</td>\n      <td>Barclays</td>\n      <td>Barclays Group Finance Director Chris Lucas is...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>662822312</td>\n      <td>False</td>\n      <td>finalized</td>\n      <td>3</td>\n      <td>2015-02-18T06:48:00</td>\n      <td>Information</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>360385000000000000</td>\n      <td>Barclays</td>\n      <td>Barclays announces that Irene McDermott Brown ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# see head of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3118, 11)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# observe shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Information    2129\n",
       "Action          724\n",
       "Dialogue        226\n",
       "Exclude          39\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# check distribution of target column i.e. category\n",
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0000    2430\n",
       "0.6614      35\n",
       "0.6643      33\n",
       "0.6747      32\n",
       "0.6775      29\n",
       "0.6662      24\n",
       "0.6668      23\n",
       "0.6663      21\n",
       "0.6675      21\n",
       "0.6743      18\n",
       "0.6639      17\n",
       "0.6763      17\n",
       "0.6634      13\n",
       "0.6573      13\n",
       "0.3386      13\n",
       "0.3418      12\n",
       "0.6602      11\n",
       "0.6631      11\n",
       "0.6731      11\n",
       "0.6569      10\n",
       "0.6582      10\n",
       "0.6496      10\n",
       "0.6809       7\n",
       "0.6645       7\n",
       "0.6728       7\n",
       "0.6460       7\n",
       "0.6647       6\n",
       "0.6677       6\n",
       "0.6811       5\n",
       "0.6656       5\n",
       "          ... \n",
       "0.7555       1\n",
       "0.8726       1\n",
       "0.6609       1\n",
       "0.8898       1\n",
       "0.9041       1\n",
       "0.7119       1\n",
       "0.6612       1\n",
       "0.8994       1\n",
       "0.8390       1\n",
       "0.6627       1\n",
       "0.6616       1\n",
       "0.7489       1\n",
       "0.3525       1\n",
       "0.7802       1\n",
       "0.5775       1\n",
       "0.8973       1\n",
       "0.7632       1\n",
       "0.6638       1\n",
       "0.8530       1\n",
       "0.6689       1\n",
       "0.8749       1\n",
       "0.5728       1\n",
       "0.8791       1\n",
       "0.9105       1\n",
       "0.6605       1\n",
       "0.8547       1\n",
       "0.6641       1\n",
       "0.8578       1\n",
       "0.9089       1\n",
       "0.8245       1\n",
       "Name: category_confidence, Length: 194, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# check distribution of the column - category_confidence\n",
    "data['category_confidence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those observations where category_confidence < 1 and category = Exclude\n",
    "data = data[(data['category_confidence']==1) & (data['category'] != 'Exclude')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0    2403\n",
       "Name: category_confidence, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# confirm distribution of the column - category_confidence\n",
    "data['category_confidence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features i.e the column - text and target i.e the column - category\n",
    "features = data['text']\n",
    "target = data['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's observe a text in the dataset, extract the first text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract the third text from this dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the below pre-processing tasks on the text \n",
    "- tokenizing the sentences\n",
    "- replace the urls with a placeholder\n",
    "- removing non ascii characters\n",
    "- text normalizing using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re library for regular expressions\n",
    "\n",
    "\n",
    "# import nltk library\n",
    "\n",
    "\n",
    "# import stopwords from nltk library\n",
    "\n",
    "\n",
    "# download the stopwords and wordnet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# extract the english stopwords and save it to a variable\n",
    "\n",
    "\n",
    "# import word_tokenize from nltk library\n",
    "\n",
    "\n",
    "# import WordNetLemmatizer from nltk library\n",
    "\n",
    "\n",
    "# write a regular expression to identify urls in text\n",
    "url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# write a regular expression to identify non-ascii characters in text\n",
    "non_ascii_regex = r'[^\\x00-\\x7F]+'\n",
    "\n",
    "# write a function to tokenize text after performing preprocessing \n",
    "def tokenize(text):\n",
    "    \n",
    "    # use library re to replace urls by token - urlplaceholder\n",
    "    \n",
    "    \n",
    "    # use library re to replace non ascii characters by a space\n",
    "      \n",
    "\n",
    "    # use word_tokenize to tokenize the sentences\n",
    "    \n",
    "    \n",
    "    # instantiate an object of class WordNetLemmatizer\n",
    "    \n",
    "\n",
    "    # use a list comprehension to lemmatize the tokens and remove the the stopwords\n",
    "    \n",
    "\n",
    "    # return the tokens\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will do exploratory data analysis to check if there is any new feature that we can generate based on the existing text that we have in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis 1:** The length of the text in each category might be different from each other\n",
    "<br>**Hypothesis 2:** The total number of URLs that are present in text might be different in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the original dataset - 'length' to capture length of each text\n",
    "\n",
    "\n",
    "# use seaborn boxplot to visualize the pattern in length for each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the original dataset - 'url_count' to capture total count of urls present in each text\n",
    "\n",
    "\n",
    "# use pandas crosstab to see the distibution of different url counts in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Creating custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimator is any object that learns from data, whether it's a classification, regression, or clustering algorithm, or a transformer that extracts or filters useful features from raw data. Since estimators learn from data, they each must have a `fit` method that takes a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of estimators - `Transformer Estimators` i.e. transformers in short and `Predictor Estimators` i.e. predictor in short. In transformers we also need to have another method `transform` and predictors need to have another method `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of `transformers` are - CountVectorizer, TfidfVectorizer, MinMaxScaler, StandardScaler etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of `predictors` are - LinearRegression, LogisticRegression, RandomForestClassifier etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom transformer LengthExtractor to extract length of each sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom transformer UrlCounter to count number of urls in each sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Model Building using FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature union applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pipeline.png \"nlp pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RandomForestClassifier from sklearn\n",
    "\n",
    "\n",
    "# import Pipeline and FeatureUnion from sklearn\n",
    "\n",
    "\n",
    "# import CountVectorizer, TfidfTransformer from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of Pipeline class\n",
    "\n",
    "    \n",
    "        # create a FeatureUnion pipeline\n",
    "        \n",
    "\n",
    "            # add a pipeline element to extract features using CountVectorizer and TfidfTransformer\n",
    "            \n",
    "\n",
    "            # add the pipeline element - LengthExtractor to extract lenght of each sentence as feature\n",
    "            \n",
    "            \n",
    "            # add another pipeline element - UrlCounter to extract url counts in each sentence as feature\n",
    "            \n",
    "\n",
    "        # use the predictor estimator RandomForestClassifier to train the model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pipeline.fit method to train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once the model is trained, in this task we will evaluate how the model behaves in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the method pipeline.predict on X_test data to predict the labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the confustion matrix, import confusion_matrix from sklearn\n",
    "\n",
    "\n",
    "# count the number of labels\n",
    "\n",
    "\n",
    "# use sns.heatmap on top of confusion_matrix to show the confusuin matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classification report, import classification_report from sklearn\n",
    "\n",
    "\n",
    "# apply the function classification_report on y_test, y_pred and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Conclusion and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to improve this model - \n",
    "\n",
    "- hyper parameter tuning\n",
    "- more feature engineering\n",
    "- feature selection\n",
    "- trying different predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}